{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "Given a set of words, we will try to construct the relevant topic from the words.\n",
    "\n",
    "The general idea is:\n",
    "1. We will create a bag of words\n",
    "2. Using respective algorithms for each method (NMF or LDA) we will generate topics\n",
    "3. We will calculate how often a collection of word show up in a topic and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KARACHI: The Sindh government has decided to b...</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>sindh govt decides to cut public transport far...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HONG KONG: Asian markets started 2015 on an up...</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>asia stocks up in new year trad</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HONG KONG:  Hong Kong shares opened 0.66 perce...</td>\n",
       "      <td>1/5/2015</td>\n",
       "      <td>hong kong stocks open 0.66 percent lower</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HONG KONG: Asian markets tumbled Tuesday follo...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>asian stocks sink euro near nine year</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK: US oil prices Monday slipped below $...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>us oil prices slip below 50 a barr</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>strong&gt;DUBAI: Dubai International Airport and ...</td>\n",
       "      <td>3/25/2017</td>\n",
       "      <td>Laptop ban hits Dubai for 11m weekend traveller</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>strong&gt;BEIJING: Former Prime Minister, Shaukat...</td>\n",
       "      <td>3/26/2017</td>\n",
       "      <td>Pak China relations not against any third coun...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>strong&gt;WASHINGTON: Uber has grounded its fleet...</td>\n",
       "      <td>3/26/2017</td>\n",
       "      <td>Uber grounds self driving cars after accid</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>strong&gt;BEIJING: The New Development Bank plans...</td>\n",
       "      <td>3/27/2017</td>\n",
       "      <td>New Development Bank plans joint investments i...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>strong&gt;KARACHI: Karachi-based technology incub...</td>\n",
       "      <td>3/27/2017</td>\n",
       "      <td>Google powered Startup Weekend energizing prou...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2692 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Article       Date  \\\n",
       "0     KARACHI: The Sindh government has decided to b...   1/1/2015   \n",
       "1     HONG KONG: Asian markets started 2015 on an up...   1/2/2015   \n",
       "2     HONG KONG:  Hong Kong shares opened 0.66 perce...   1/5/2015   \n",
       "3     HONG KONG: Asian markets tumbled Tuesday follo...   1/6/2015   \n",
       "4     NEW YORK: US oil prices Monday slipped below $...   1/6/2015   \n",
       "...                                                 ...        ...   \n",
       "2687  strong>DUBAI: Dubai International Airport and ...  3/25/2017   \n",
       "2688  strong>BEIJING: Former Prime Minister, Shaukat...  3/26/2017   \n",
       "2689  strong>WASHINGTON: Uber has grounded its fleet...  3/26/2017   \n",
       "2690  strong>BEIJING: The New Development Bank plans...  3/27/2017   \n",
       "2691  strong>KARACHI: Karachi-based technology incub...  3/27/2017   \n",
       "\n",
       "                                                Heading  NewsType  \n",
       "0     sindh govt decides to cut public transport far...  business  \n",
       "1                       asia stocks up in new year trad  business  \n",
       "2              hong kong stocks open 0.66 percent lower  business  \n",
       "3                asian stocks sink euro near nine year   business  \n",
       "4                    us oil prices slip below 50 a barr  business  \n",
       "...                                                 ...       ...  \n",
       "2687    Laptop ban hits Dubai for 11m weekend traveller  business  \n",
       "2688  Pak China relations not against any third coun...  business  \n",
       "2689         Uber grounds self driving cars after accid  business  \n",
       "2690  New Development Bank plans joint investments i...  business  \n",
       "2691  Google powered Startup Weekend energizing prou...  business  \n",
       "\n",
       "[2692 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/Articles.csv\", sep=\",\", encoding= 'unicode_escape')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "For LDA, we only need the bag of words, so we use Count Vectorizer\n",
    "For NMF, we need the Tf-Idf values because of linear algebra, so we use Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiv = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.95)\n",
    "df_vectorized = tiv.fit_transform(df['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2692x29388 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 350156 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=\"english\", min_df=1, max_df=0.95)\n",
    "cv_vectorized = cv.fit_transform(df['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', ..., '½zaman', '½zarai', '½ï'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "Considers P(topic | document) * P(word | topic). Randomly assigns a word to a topic and then evaluate how often the word and other words show up in that topic. Take the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=10)\n",
    "final_topics = LDA.fit_transform(cv_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the topics (clusters) along with the words that are in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\n",
      "['gold', 'bank', 'yen', 'week', 'markets', 'year', 'said', '½s', 'dollar', 'percent']\n",
      "Topic #1\n",
      "['market', 'output', 'barrel', 'production', 'million', 'opec', 'prices', 'crude', 'said', 'oil']\n",
      "Topic #2\n",
      "['government', 'economic', 'china', 'minister', 'percent', 'year', 'billion', '½s', 'pakistan', 'said']\n",
      "Topic #3\n",
      "['match', 'series', 'wicket', 'south', 'ball', 'innings', 'second', 'test', 'england', '½s']\n",
      "Topic #4\n",
      "['game', 'germany', 'federer', 'second', 'strong', 'set', 'ireland', 'final', 'match', 'france']\n",
      "Topic #5\n",
      "['told', 'games', 'br', 'team', 'strong', 'time', 'year', 'world', 'said', '½s']\n",
      "Topic #6\n",
      "['½ï', 'test', 'new', 'captain', 'team', 'india', 'match', '½s', 'pakistan', 'cricket']\n",
      "Topic #7\n",
      "['period', 'compared', 'strong', '2016', 'exports', '2015', 'united', 'fifa', 'year', 'million']\n",
      "Topic #8\n",
      "['petrol', 'new', 'petroleum', 'litre', 'strong', 'rs', 'islamabad', 'prices', 'price', 'said']\n",
      "Topic #9\n",
      "['½s', 'million', 'minister', 'board', 'security', 'rs', 'government', 'billion', 'tax', 'said']\n"
     ]
    }
   ],
   "source": [
    "for id, topic in enumerate(LDA.components_):\n",
    "    print(f'Topic #{id}')\n",
    "    print([top for top in cv.get_feature_names_out()[topic.argsort()[-10:]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity of each sentence to each sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00144947, 0.0014496 , 0.00144955, 0.00144939, 0.00144951,\n",
       "       0.00144938, 0.00144938, 0.0014494 , 0.41098109, 0.57742324])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'] = final_topics.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    723\n",
       "2    483\n",
       "0    306\n",
       "5    282\n",
       "1    242\n",
       "3    235\n",
       "9    142\n",
       "8    121\n",
       "7     92\n",
       "4     66\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF\n",
    "Input a weighted matrix (V) to determine H (the topics and the documents) using W (the words and the topics) which gives V ~ W * H.\n",
    "The column for W and H represents the topics. That is why when V is m x n then W is m x k and H is k x n, with k is the number of topics that we want to generate.\n",
    "\n",
    "So in the end, here we are trying to maximize the coefficient instead of the probability like in LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10)\n",
    "final_topics_nmf = nmf.fit_transform(df_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\n",
      "['week', 'fed', 'rates', 'bank', 'markets', 'yen', 'rate', 'gold', 'dollar', 'percent']\n",
      "Topic #1\n",
      "['india', 'indies', 'balls', 'australia', 'wicket', 'overs', 'lanka', 'wickets', 'sri', 'runs']\n",
      "Topic #2\n",
      "['million', 'barrels', 'cents', 'output', 'production', 'barrel', 'opec', 'prices', 'crude', 'oil']\n",
      "Topic #3\n",
      "['½s', 'finance', 'imf', 'economic', 'tax', 'government', 'minister', 'pakistan', 'said', 'billion']\n",
      "Topic #4\n",
      "['dubai', 'pakistan', 'captain', 'quetta', 'ahmed', 'kings', 'gladiators', 'peshawar', 'zalmi', 'mohammad']\n",
      "Topic #5\n",
      "['ball', 'lordï', 'root', 'amir', 'series', 'innings', 'cook', '½s', 'england', 'test']\n",
      "Topic #6\n",
      "['security', 'icc', 'players', 'board', 'said', 'pcb', 'team', 'india', 'pakistan', 'cricket']\n",
      "Topic #7\n",
      "['year', 'title', 'win', 'messi', 'said', 'open', 'time', 'world', 'final', '½s']\n",
      "Topic #8\n",
      "['oil', 'regulatory', 'prices', 'petroleum', 'ogra', 'price', 'petrol', 'diesel', 'rs', 'litre']\n",
      "Topic #9\n",
      "['benchmark', 'topix', '225', 'shares', 'stocks', 'yen', 'percent', 'tokyo', 'index', 'points']\n"
     ]
    }
   ],
   "source": [
    "for id, topic in enumerate(nmf.components_):\n",
    "    print(f'Topic #{id}')\n",
    "    print([top for top in tiv.get_feature_names_out()[topic.argsort()[-10:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_nmf'] = final_topics_nmf.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    595\n",
       "7    493\n",
       "6    328\n",
       "1    315\n",
       "2    256\n",
       "5    190\n",
       "0    185\n",
       "4    120\n",
       "9    107\n",
       "8    103\n",
       "Name: topic_nmf, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic_nmf'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
